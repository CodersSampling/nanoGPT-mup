{
  "out_dir": "coord_check/sp_with_mup_hidden_init_and_lr/out/width2048_depth2_seed1",
  "eval_interval": 1,
  "log_interval": 1,
  "eval_iters": 1,
  "eval_only": false,
  "always_save_checkpoint": false,
  "never_save_checkpoint": true,
  "init_from": "scratch",
  "wandb_log": false,
  "wandb_project": "owt",
  "wandb_run_name": "gpt2",
  "csv_log": true,
  "dataset": "shakespeare_char",
  "gradient_accumulation_steps": 4,
  "batch_size": 2,
  "block_size": 1024,
  "n_layer": 2,
  "n_head": 32,
  "n_embd": 2048,
  "dropout": 0.0,
  "bias": false,
  "init_std": 0.02,
  "learning_rate": 0.01,
  "max_iters": 10,
  "weight_decay": 0.1,
  "beta1": 0.9,
  "beta2": 0.95,
  "grad_clip": 1.0,
  "decay_lr": false,
  "warmup_iters": 2000,
  "lr_decay_iters": 600000,
  "min_lr": 6e-05,
  "mup_enabled": true,
  "mup_disable_attention_scaling": true,
  "mup_disable_hidden_lr_scaling": false,
  "mup_width_multiplier": 8,
  "mup_input_alpha": 1,
  "mup_output_alpha": 8,
  "mup_enable_coord_check_logging": true,
  "seed": 1,
  "backend": "nccl",
  "device": "mps",
  "dtype": "float32",
  "compile": false
}